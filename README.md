# Large-Language-Model-Yi
Prompt Engineering refers to the development and optimization of prompts that interact with LLM to guide it to produce the desired results without the need for model updates. Prompt Engineering can help researchers improve the ability of large language models to handle complex tasks, such as question answering and arithmetic reasoning, or enhance the performance and effectiveness of generative AI models in specific task scenarios.

There are four important elements in the writing of prompts:

1: Instruction: Clearly specify the specific task or instruction that the language model is expected to perform.

2: Context: Providing external information or additional context to guide language models to better understand and respond.

3: Input data: including user input content or questions, as the basis for generating output for the model.

4: Output indication: Specify the expected output type or format.

I conducted in-depth research on the Yi series models while being exposed to different large language models like Qwen, Chatglm, Bluelm and so on. I have gained a deep understanding of prompt engineering and how to rewrite prompts to have a better output(in terms of precision, accuracy and recall rates). And when writing prompt and refining it, I realize that language effiency also matters like English and Chinese will have different output with the same question.

In order to let our model to generate better output, we will train our model. As we train it more it will become better at performance. We also need to validate our outcome, and finally we will have to test our model. Just like we as students doing practices everyday(Training), and then we will have midterm exams to check our practice effort(validation), finally we will have final exam(Testing). Model just like us but not that smart. And thats why we are needed to split dataset into Training DataSet, Validation DataSet and Testing DataSet.
